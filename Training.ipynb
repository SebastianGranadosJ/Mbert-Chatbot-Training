{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9083cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import evaluate\n",
    "from rapidfuzz import fuzz\n",
    "import json\n",
    "import unicodedata\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5189b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_similar(text_list, threshold=95):\n",
    "    \"\"\"\n",
    "    Removes elements that are highly similar to each other.\n",
    "    threshold: similarity percentage (0–100)\n",
    "    \"\"\"\n",
    "    unique_texts = []\n",
    "    for text in text_list:\n",
    "        is_duplicate = False\n",
    "        for u_text in unique_texts:\n",
    "            # Levenshtein similarity ratio\n",
    "            if fuzz.ratio(text, u_text) >= threshold:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        if not is_duplicate:\n",
    "            unique_texts.append(text)\n",
    "    return unique_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c75d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy models for Spanish and English\n",
    "# Since the chatbot's training corpus contains both Spanish and English data,\n",
    "# we need separate lemmatization dictionaries for each language.\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c2460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo normalizado guardado en ./faqs/faqs_normalized.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define paths for the original FAQ file and the normalized output file\n",
    "FAQ_PATH = \"./faqs/faqs.json\"\n",
    "OUTPUT_PATH = \"./faqs/faqs_normalized.json\"\n",
    "\n",
    "# Load the raw FAQ data\n",
    "with open(FAQ_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    faq_data = json.load(f)[\"faqs\"]\n",
    "\n",
    "def normalize_text(text: str, lang: str = \"es\") -> str:\n",
    "    \"\"\"\n",
    "    Normalizes a text string by applying:\n",
    "    - Lowercasing\n",
    "    - Removing question marks\n",
    "    - Accent removal\n",
    "    - Lemmatization\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove question marks (both Spanish and English forms)\n",
    "    text = text.replace(\"¿\", \"\").replace(\"?\", \"\")\n",
    "    \n",
    "    # Lowercase again and remove accents/diacritics\n",
    "    text = text.lower()\n",
    "    text = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "    # Select the spaCy model depending on the language\n",
    "    # Since the dataset contains Spanish and English questions,\n",
    "    # each language must use its own lemmatization dictionary.\n",
    "    nlp = nlp_es if lang == \"es\" else nlp_en\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Apply lemmatization token by token\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "    # Remove unnecessary double spaces\n",
    "    lemmatized = \" \".join(lemmatized.split())\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "# Process and normalize all questions for each FAQ entry\n",
    "for faq in faq_data:\n",
    "    for lang in [\"es\", \"en\"]:\n",
    "        if lang in faq[\"questions\"]:\n",
    "            # Apply text normalization to each question\n",
    "            normalized = [normalize_text(q, lang) for q in faq[\"questions\"][lang]]\n",
    "            \n",
    "            # Remove exact duplicates while preserving order\n",
    "            normalized = list(dict.fromkeys(normalized))\n",
    "            \n",
    "            # Remove near-duplicate questions (similarity > 95%)\n",
    "            normalized = remove_similar(normalized, threshold=95)\n",
    "            \n",
    "            faq[\"questions\"][lang] = normalized\n",
    "\n",
    "# Save the normalized FAQ dataset\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"faqs\": faq_data}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Archivo normalizado guardado en {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 1. Load normalized FAQs\n",
    "# =========================================================\n",
    "FAQ_PATH = \"./faqs/faqs_normalized.json\"\n",
    "\n",
    "# Read the normalized FAQ dataset\n",
    "with open(FAQ_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    faq_data = json.load(f)[\"faqs\"]\n",
    "\n",
    "# Prepare containers for questions, labels, and label mappings\n",
    "questions = []\n",
    "labels = []\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "\n",
    "# Create label mappings and expand all Spanish and English questions\n",
    "for idx, faq in enumerate(faq_data):\n",
    "    # Map internal index → FAQ ID and FAQ ID → internal index\n",
    "    label2id[str(idx)] = faq[\"id\"]\n",
    "    id2label[str(faq[\"id\"])] = idx\n",
    "\n",
    "    # Detect whether the FAQ uses the old format (\"question\") or the new one (\"questions\")\n",
    "    q_data = faq.get(\"questions\") or faq.get(\"question\")\n",
    "\n",
    "    for lang in [\"es\", \"en\"]:\n",
    "        if lang in q_data:\n",
    "            # If the entry is a string (old format), convert it into a list\n",
    "            q_list = q_data[lang] if isinstance(q_data[lang], list) else [q_data[lang]]\n",
    "\n",
    "            # Add each question to the dataset, associated with its label\n",
    "            for q in q_list:\n",
    "                questions.append(q)\n",
    "                labels.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\torch_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5eea1aa9fe423ebdd4146004388337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d339bd1b5e4b8ab282325142ca40ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/266 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 2. Tokenization\n",
    "# =========================================================\n",
    "MODEL_NAME = \"bert-base-multilingual-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    # Tokenize each text input using the multilingual BERT tokenizer\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "# Number of unique classes (each distinct FAQ entry is its own class)\n",
    "num_classes = len(set(labels))\n",
    "\n",
    "# Define the dataset features for Hugging Face Datasets\n",
    "features = Features({\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"label\": ClassLabel(num_classes=num_classes)\n",
    "})\n",
    "\n",
    "# =========================================================\n",
    "# 3. Create dataset with stratification\n",
    "# =========================================================\n",
    "# Each different question corresponds to a separate class.\n",
    "# Stratification ensures that all classes remain properly represented\n",
    "# in both the training and test splits.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    questions,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,      # <- stratify by class label\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Build Hugging Face Datasets for train/test\n",
    "train_data = Dataset.from_dict({\"text\": X_train, \"label\": y_train}, features=features)\n",
    "test_data  = Dataset.from_dict({\"text\": X_test, \"label\": y_test}, features=features)\n",
    "\n",
    "# Tokenize both datasets\n",
    "train_dataset = train_data.map(tokenize, batched=True)\n",
    "test_dataset  = test_data.map(tokenize, batched=True)\n",
    "\n",
    "# Format datasets for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# =========================================================\n",
    "# 4. Define the model\n",
    "# =========================================================\n",
    "num_labels = num_classes\n",
    "\n",
    "# Detect GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load a multilingual BERT model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 5. Training configuration\n",
    "# =========================================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./faq_model_2\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=8,   # increased training epochs for better performance\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Accuracy metric\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "# =========================================================\n",
    "# 6. Trainer\n",
    "# =========================================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71951c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6539bc97e155401daf33b866ba054413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4669dd871e0e461d8464ed108aa1ec8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.291703939437866, 'eval_accuracy': 0.22180451127819548, 'eval_runtime': 11.5192, 'eval_samples_per_second': 23.092, 'eval_steps_per_second': 1.476, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0085d0f5f24ee099b9292eee699344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5876176357269287, 'eval_accuracy': 0.5977443609022557, 'eval_runtime': 10.635, 'eval_samples_per_second': 25.012, 'eval_steps_per_second': 1.598, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceebcceb356b4313bd4ae1fc33d921aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9940024614334106, 'eval_accuracy': 0.7180451127819549, 'eval_runtime': 11.5092, 'eval_samples_per_second': 23.112, 'eval_steps_per_second': 1.477, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e014cb57294fcf8fd561b0d2eb60a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5976312160491943, 'eval_accuracy': 0.8120300751879699, 'eval_runtime': 10.6237, 'eval_samples_per_second': 25.038, 'eval_steps_per_second': 1.6, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26921335111a4c88a9a765ae4489f4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3220584392547607, 'eval_accuracy': 0.8308270676691729, 'eval_runtime': 10.311, 'eval_samples_per_second': 25.798, 'eval_steps_per_second': 1.649, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78487f44050498ca0ca9cf016252f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1496819257736206, 'eval_accuracy': 0.868421052631579, 'eval_runtime': 10.675, 'eval_samples_per_second': 24.918, 'eval_steps_per_second': 1.593, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9578e8d9f44f4d518397f1dbac4bafaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0533047914505005, 'eval_accuracy': 0.8834586466165414, 'eval_runtime': 10.5794, 'eval_samples_per_second': 25.143, 'eval_steps_per_second': 1.607, 'epoch': 7.0}\n",
      "{'loss': 2.0333, 'grad_norm': 7.578559875488281, 'learning_rate': 1.3432835820895524e-06, 'epoch': 7.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b3cc4c1d1c4f768f27edd975f3d019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0237741470336914, 'eval_accuracy': 0.8796992481203008, 'eval_runtime': 8.8997, 'eval_samples_per_second': 29.889, 'eval_steps_per_second': 1.91, 'epoch': 8.0}\n",
      "{'train_runtime': 2878.1513, 'train_samples_per_second': 2.949, 'train_steps_per_second': 0.186, 'train_loss': 1.9653314405412816, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./faq_model_2\\\\tokenizer_config.json',\n",
       " './faq_model_2\\\\special_tokens_map.json',\n",
       " './faq_model_2\\\\vocab.txt',\n",
       " './faq_model_2\\\\added_tokens.json',\n",
       " './faq_model_2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 7. Train the model\n",
    "# =========================================================\n",
    "# Start the training process using the Trainer configuration.\n",
    "# This will run the training loop, evaluation per epoch,\n",
    "# and automatically store the best model if configured.\n",
    "trainer.train()\n",
    "\n",
    "# =========================================================\n",
    "# 8. Save the trained model\n",
    "# =========================================================\n",
    "# After training finishes, save both the model and tokenizer\n",
    "# so they can be loaded later for inference or deployment.\n",
    "model.save_pretrained(\"./faq_model_2\")\n",
    "tokenizer.save_pretrained(\"./faq_model_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG Prediction: [{'label': 'LABEL_23', 'score': 0.4423360526561737}]\n",
      "{'0': 1, '1': 2, '2': 4, '3': 6, '4': 9, '5': 10, '6': 11, '7': 12, '8': 15, '9': 17, '10': 26, '11': 28, '12': 30, '13': 33, '14': 36, '15': 37, '16': 41, '17': 53, '18': 54, '19': 55, '20': 62, '21': 67, '22': 69, '23': 72, '24': 100, '25': 101, '26': 102, '27': 605, '28': 200, '29': 201, '30': 202, '31': 203, '32': 204, '33': 300, '34': 301, '35': 400, '36': 401, '37': 402, '38': 403, '39': 500, '40': 503, '41': 504, '42': 600}\n",
      "{'1': 0, '2': 1, '4': 2, '6': 3, '9': 4, '10': 5, '11': 6, '12': 7, '15': 8, '17': 9, '26': 10, '28': 11, '30': 12, '33': 13, '36': 14, '37': 15, '41': 16, '53': 17, '54': 18, '55': 19, '62': 20, '67': 21, '69': 22, '72': 23, '100': 24, '101': 25, '102': 26, '605': 27, '200': 28, '201': 29, '202': 30, '203': 31, '204': 32, '300': 33, '301': 34, '400': 35, '401': 36, '402': 37, '403': 38, '500': 39, '503': 40, '504': 41, '600': 42}\n",
      "Pregunta original: que es el SP 500\n",
      "Respuesta: {'es': 'El S&P 500 es un índice bursátil que agrupa a 500 de las empresas más grandes y representativas de Estados Unidos, elegidas por su tamaño de mercado, liquidez y relevancia en la economía. Se utiliza como un termómetro del mercado estadounidense, ya que refleja de manera bastante amplia el comportamiento de distintos sectores. Es una de las referencias más importantes para inversores y analistas, junto con otros índices como el Dow Jones o el Nasdaq.', 'en': 'The S&P 500 is a stock market index that includes 500 of the largest and most representative U.S. companies, selected based on market size, liquidity, and economic relevance. It is widely used as a barometer of the U.S. market, as it broadly reflects the performance of different sectors. Along with indices like the Dow Jones and the Nasdaq, it is one of the most important benchmarks for investors and analysts.'}\n",
      "Confianza: 44.23%\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# --- Load spaCy models for lemmatization ---\n",
    "# We load both Spanish and English models because the chatbot\n",
    "# needs to normalize and lemmatize user input depending on the language.\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")  # Spanish\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")   # English\n",
    "\n",
    "# --- Text normalization function ---\n",
    "def normalize_text(text: str, lang: str = \"es\") -> str:\n",
    "    \"\"\"\n",
    "    Normalize text before feeding it to the classifier:\n",
    "    - Convert to lowercase\n",
    "    - Remove question marks\n",
    "    - Remove accents\n",
    "    - Lemmatize using the appropriate language model\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove question marks\n",
    "    text = text.replace(\"¿\", \"\").replace(\"?\", \"\")\n",
    "    \n",
    "    # Remove accents/diacritics\n",
    "    text = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "    # Lemmatization\n",
    "    nlp = nlp_es if lang == \"es\" else nlp_en\n",
    "    doc = nlp(text)\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "    # Remove double spaces\n",
    "    lemmatized = \" \".join(lemmatized.split())\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "# --- Load trained model and tokenizer ---\n",
    "# These are the artifacts saved after fine-tuning the multilingual BERT model.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./faq_model_2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./faq_model_2\")\n",
    "\n",
    "# --- Create inference pipeline ---\n",
    "# The Hugging Face pipeline simplifies running predictions on text.\n",
    "clf = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=0 if torch.cuda.is_available() else -1   # Use GPU if available\n",
    ")\n",
    "\n",
    "# --- Prediction function with normalization ---\n",
    "def get_answer(user_question: str, lang: str = \"es\"):\n",
    "    # Normalize user input before passing it to the model\n",
    "    clean_q = normalize_text(user_question, lang=lang)\n",
    "    \n",
    "    # Run model inference\n",
    "    pred = clf(clean_q, truncation=True, max_length=64)[0]\n",
    "    \n",
    "    # Extract predicted class index and confidence score\n",
    "    label_idx = int(pred[\"label\"].replace(\"LABEL_\", \"\"))\n",
    "    confidence = pred[\"score\"]\n",
    "    \n",
    "    # Retrieve original FAQ ID from mapping\n",
    "    faq_id = label2id[str(label_idx)]\n",
    "    \n",
    "    # Look up the corresponding answer in the JSON data\n",
    "    for faq in faq_data:\n",
    "        if faq[\"id\"] == faq_id:\n",
    "            return faq.get(\"answer\", \"No answer available.\"), confidence\n",
    "    \n",
    "    return \"I couldn't find an answer.\", confidence\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Example usage\n",
    "# =========================================================\n",
    "pregunta = \"que es el SP 500\"\n",
    "respuesta, confianza = get_answer(pregunta, lang=\"es\")\n",
    "\n",
    "print(f\"Pregunta original: {pregunta}\")\n",
    "print(f\"Respuesta: {respuesta}\")\n",
    "print(f\"Confianza: {confianza:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba981d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
