{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9083cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "import evaluate\n",
    "from rapidfuzz import fuzz\n",
    "import json\n",
    "import unicodedata\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5189b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_similar(text_list, threshold=95):\n",
    "    \"\"\"\n",
    "    Elimina elementos muy similares entre sí.\n",
    "    threshold: porcentaje de similitud (0-100)\n",
    "    \"\"\"\n",
    "    unique_texts = []\n",
    "    for text in text_list:\n",
    "        is_duplicate = False\n",
    "        for u_text in unique_texts:\n",
    "            # ratio de similitud de Levenshtein\n",
    "            if fuzz.ratio(text, u_text) >= threshold:\n",
    "                is_duplicate = True\n",
    "                break\n",
    "        if not is_duplicate:\n",
    "            unique_texts.append(text)\n",
    "    return unique_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c75d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar spaCy para español e inglés\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43c2460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo normalizado guardado en ./faqs/faqs_normalized.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ruta del archivo original\n",
    "FAQ_PATH = \"./faqs/faqs.json\"\n",
    "OUTPUT_PATH = \"./faqs/faqs_normalized.json\"\n",
    "\n",
    "# Cargar datos\n",
    "with open(FAQ_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    faq_data = json.load(f)[\"faqs\"]\n",
    "\n",
    "def normalize_text(text: str, lang: str = \"es\") -> str:\n",
    "    \"\"\"\n",
    "    Normaliza el texto:\n",
    "    - Minusculas\n",
    "    - Elimina signos de interrogación\n",
    "    - Lematización\n",
    "    \"\"\"\n",
    "    # Minusculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminar signos de interrogación\n",
    "    text = text.replace(\"¿\", \"\").replace(\"?\", \"\")\n",
    "    \n",
    "    # Minúsculas y eliminar tildes\n",
    "    text = text.lower()\n",
    "    text = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "    # Lematización\n",
    "    nlp = nlp_es if lang == \"es\" else nlp_en\n",
    "    doc = nlp(text)\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "    # Eliminar espacios dobles\n",
    "    lemmatized = \" \".join(lemmatized.split())\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "# Procesar todas las preguntas\n",
    "for faq in faq_data:\n",
    "    for lang in [\"es\", \"en\"]:\n",
    "        if lang in faq[\"questions\"]:\n",
    "            # Normalización previa (minúsculas, lemmatización, quitar signos)\n",
    "            normalized = [normalize_text(q, lang) for q in faq[\"questions\"][lang]]\n",
    "            \n",
    "            # Eliminar duplicados exactos\n",
    "            normalized = list(dict.fromkeys(normalized))\n",
    "            \n",
    "            # Eliminar duplicados similares >95%\n",
    "            normalized = remove_similar(normalized, threshold=95)\n",
    "            \n",
    "            faq[\"questions\"][lang] = normalized\n",
    "\n",
    "# Guardar JSON normalizado\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"faqs\": faq_data}, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Archivo normalizado guardado en {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a61c38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================\n",
    "# 1. Cargar FAQs\n",
    "# =========================================================\n",
    "FAQ_PATH =  \"./faqs/faqs_normalized.json\"\n",
    "\n",
    "with open(FAQ_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    faq_data = json.load(f)[\"faqs\"]\n",
    "\n",
    "# Preparamos preguntas y etiquetas\n",
    "questions = []\n",
    "labels = []\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "\n",
    "# Crear mappings y expandir todas las preguntas en español e inglés\n",
    "for idx, faq in enumerate(faq_data):\n",
    "    label2id[str(idx)] = faq[\"id\"]\n",
    "    id2label[str(faq[\"id\"])] = idx\n",
    "\n",
    "    # Detectar formato: viejo (\"question\") o nuevo (\"questions\")\n",
    "    q_data = faq.get(\"questions\") or faq.get(\"question\")\n",
    "\n",
    "    for lang in [\"es\", \"en\"]:\n",
    "        if lang in q_data:\n",
    "            # Si es string (viejo formato), convertir a lista\n",
    "            q_list = q_data[lang] if isinstance(q_data[lang], list) else [q_data[lang]]\n",
    "\n",
    "            for q in q_list:\n",
    "                questions.append(q)\n",
    "                labels.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\torch_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5eea1aa9fe423ebdd4146004388337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d339bd1b5e4b8ab282325142ca40ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/266 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. Tokenización\n",
    "# =========================================================\n",
    "MODEL_NAME = \"bert-base-multilingual-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=64\n",
    "    )\n",
    "\n",
    "num_classes = len(set(labels))\n",
    "features = Features({\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"label\": ClassLabel(num_classes=num_classes)\n",
    "})\n",
    "\n",
    "# =========================================================\n",
    "# 3. Crear dataset con estratificación\n",
    "# =========================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    questions,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,      # <- estratificación por clase\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_data = Dataset.from_dict({\"text\": X_train, \"label\": y_train}, features=features)\n",
    "test_data  = Dataset.from_dict({\"text\": X_test, \"label\": y_test}, features=features)\n",
    "\n",
    "# Tokenizar datasets\n",
    "train_dataset = train_data.map(tokenize, batched=True)\n",
    "test_dataset  = test_data.map(tokenize, batched=True)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# =========================================================\n",
    "# 4. Definir modelo\n",
    "# =========================================================\n",
    "# -------- 4. Modelo --------\n",
    "num_labels = num_classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # <-- detectar GPU\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# -------- 5. Configuración de entrenamiento --------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./faq_model_2\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs_2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=8,   # <-- aquí aumenté de 3 a 8 épocas\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "\n",
    "# Métricas\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=p.label_ids)\n",
    "\n",
    "# =========================================================\n",
    "# 6. Entrenador\n",
    "# =========================================================\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e71951c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6539bc97e155401daf33b866ba054413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4669dd871e0e461d8464ed108aa1ec8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.291703939437866, 'eval_accuracy': 0.22180451127819548, 'eval_runtime': 11.5192, 'eval_samples_per_second': 23.092, 'eval_steps_per_second': 1.476, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0085d0f5f24ee099b9292eee699344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5876176357269287, 'eval_accuracy': 0.5977443609022557, 'eval_runtime': 10.635, 'eval_samples_per_second': 25.012, 'eval_steps_per_second': 1.598, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceebcceb356b4313bd4ae1fc33d921aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9940024614334106, 'eval_accuracy': 0.7180451127819549, 'eval_runtime': 11.5092, 'eval_samples_per_second': 23.112, 'eval_steps_per_second': 1.477, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e014cb57294fcf8fd561b0d2eb60a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5976312160491943, 'eval_accuracy': 0.8120300751879699, 'eval_runtime': 10.6237, 'eval_samples_per_second': 25.038, 'eval_steps_per_second': 1.6, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26921335111a4c88a9a765ae4489f4f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3220584392547607, 'eval_accuracy': 0.8308270676691729, 'eval_runtime': 10.311, 'eval_samples_per_second': 25.798, 'eval_steps_per_second': 1.649, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e78487f44050498ca0ca9cf016252f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1496819257736206, 'eval_accuracy': 0.868421052631579, 'eval_runtime': 10.675, 'eval_samples_per_second': 24.918, 'eval_steps_per_second': 1.593, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9578e8d9f44f4d518397f1dbac4bafaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0533047914505005, 'eval_accuracy': 0.8834586466165414, 'eval_runtime': 10.5794, 'eval_samples_per_second': 25.143, 'eval_steps_per_second': 1.607, 'epoch': 7.0}\n",
      "{'loss': 2.0333, 'grad_norm': 7.578559875488281, 'learning_rate': 1.3432835820895524e-06, 'epoch': 7.46}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b3cc4c1d1c4f768f27edd975f3d019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0237741470336914, 'eval_accuracy': 0.8796992481203008, 'eval_runtime': 8.8997, 'eval_samples_per_second': 29.889, 'eval_steps_per_second': 1.91, 'epoch': 8.0}\n",
      "{'train_runtime': 2878.1513, 'train_samples_per_second': 2.949, 'train_steps_per_second': 0.186, 'train_loss': 1.9653314405412816, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./faq_model_2\\\\tokenizer_config.json',\n",
       " './faq_model_2\\\\special_tokens_map.json',\n",
       " './faq_model_2\\\\vocab.txt',\n",
       " './faq_model_2\\\\added_tokens.json',\n",
       " './faq_model_2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 7. Entrenar modelo\n",
    "# =========================================================\n",
    "trainer.train()\n",
    "\n",
    "# =========================================================\n",
    "# 8. Guardar modelo entrenado\n",
    "# =========================================================\n",
    "model.save_pretrained(\"./faq_model_2\")\n",
    "tokenizer.save_pretrained(\"./faq_model_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG Prediction: [{'label': 'LABEL_23', 'score': 0.4423360526561737}]\n",
      "{'0': 1, '1': 2, '2': 4, '3': 6, '4': 9, '5': 10, '6': 11, '7': 12, '8': 15, '9': 17, '10': 26, '11': 28, '12': 30, '13': 33, '14': 36, '15': 37, '16': 41, '17': 53, '18': 54, '19': 55, '20': 62, '21': 67, '22': 69, '23': 72, '24': 100, '25': 101, '26': 102, '27': 605, '28': 200, '29': 201, '30': 202, '31': 203, '32': 204, '33': 300, '34': 301, '35': 400, '36': 401, '37': 402, '38': 403, '39': 500, '40': 503, '41': 504, '42': 600}\n",
      "{'1': 0, '2': 1, '4': 2, '6': 3, '9': 4, '10': 5, '11': 6, '12': 7, '15': 8, '17': 9, '26': 10, '28': 11, '30': 12, '33': 13, '36': 14, '37': 15, '41': 16, '53': 17, '54': 18, '55': 19, '62': 20, '67': 21, '69': 22, '72': 23, '100': 24, '101': 25, '102': 26, '605': 27, '200': 28, '201': 29, '202': 30, '203': 31, '204': 32, '300': 33, '301': 34, '400': 35, '401': 36, '402': 37, '403': 38, '500': 39, '503': 40, '504': 41, '600': 42}\n",
      "Pregunta original: que es el SP 500\n",
      "Respuesta: {'es': 'El S&P 500 es un índice bursátil que agrupa a 500 de las empresas más grandes y representativas de Estados Unidos, elegidas por su tamaño de mercado, liquidez y relevancia en la economía. Se utiliza como un termómetro del mercado estadounidense, ya que refleja de manera bastante amplia el comportamiento de distintos sectores. Es una de las referencias más importantes para inversores y analistas, junto con otros índices como el Dow Jones o el Nasdaq.', 'en': 'The S&P 500 is a stock market index that includes 500 of the largest and most representative U.S. companies, selected based on market size, liquidity, and economic relevance. It is widely used as a barometer of the U.S. market, as it broadly reflects the performance of different sectors. Along with indices like the Dow Jones and the Nasdaq, it is one of the most important benchmarks for investors and analysts.'}\n",
      "Confianza: 44.23%\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# --- Cargar spacy para lematización ---\n",
    "nlp_es = spacy.load(\"es_core_news_sm\")  # español\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")   # inglés\n",
    "\n",
    "# --- Función de normalización ---\n",
    "def normalize_text(text: str, lang: str = \"es\") -> str:\n",
    "    \"\"\"\n",
    "    Normaliza el texto:\n",
    "    - Pasa a minúsculas\n",
    "    - Elimina signos de interrogación\n",
    "    - Elimina tildes\n",
    "    - Lematiza\n",
    "    \"\"\"\n",
    "    # Minusculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Eliminar signos de interrogación\n",
    "    text = text.replace(\"¿\", \"\").replace(\"?\", \"\")\n",
    "    \n",
    "    # Eliminar tildes\n",
    "    text = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "    # Lematización\n",
    "    nlp = nlp_es if lang == \"es\" else nlp_en\n",
    "    doc = nlp(text)\n",
    "    lemmatized = \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "    # Quitar espacios dobles\n",
    "    lemmatized = \" \".join(lemmatized.split())\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "\n",
    "# --- Cargar modelo y tokenizer entrenados ---\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./faq_model_2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./faq_model_2\")\n",
    "\n",
    "# --- Crear pipeline ---\n",
    "clf = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# --- Función de respuesta con normalización ---\n",
    "def get_answer(user_question: str, lang: str = \"es\"):\n",
    "    # Normalizar antes de pasar al modelo\n",
    "    clean_q = normalize_text(user_question, lang=lang)\n",
    "    \n",
    "    # Obtener predicción\n",
    "    pred = clf(clean_q, truncation=True, max_length=64)[0]\n",
    "    \n",
    "    # Extraer etiqueta y confianza\n",
    "    label_idx = int(pred[\"label\"].replace(\"LABEL_\", \"\"))\n",
    "    confidence = pred[\"score\"]\n",
    "    \n",
    "    # Recuperar el ID original de FAQ\n",
    "    faq_id = label2id[str(label_idx)]\n",
    "    print(label2id)\n",
    "    print(id2label)\n",
    "    # Buscar respuesta en JSON\n",
    "    for faq in faq_data:\n",
    "        if faq[\"id\"] == faq_id:\n",
    "            return faq.get(\"answer\", \"No tengo respuesta registrada.\"), confidence\n",
    "    \n",
    "    return \"No encontré respuesta.\", confidence\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Ejemplo de uso\n",
    "# =========================================================\n",
    "# 15%\n",
    "pregunta = \"que es el SP 500\"\n",
    "respuesta, confianza = get_answer(pregunta, lang=\"es\")\n",
    "\n",
    "print(f\"Pregunta original: {pregunta}\")\n",
    "print(f\"Respuesta: {respuesta}\")\n",
    "print(f\"Confianza: {confianza:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba981d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
